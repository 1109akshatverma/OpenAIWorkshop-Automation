page-number-292-line-number-30,"'different parts that can run concurrently in different nodes, and each one will run on the'
'same data. The scalaility of this method depends on the degree of task parallelization'
'of the algorithm, and it is more complex to implement than data parallelism.'
'In model parallelism, worker nodes only need to synchronize the shared parameters,'
""usually once for each forward or ackward-propagation step. Also, larger models aren't""
'a concern since each node operates on a susection of the model on the same training'
'data.'
'Next steps'
'\xc2\x7 For a technical example, see the reference architecture scenario.'
'\xc2\x7 Find tips for MPI, TensorFlow, and PyTorch in the Distriuted GPU training guide'
"
