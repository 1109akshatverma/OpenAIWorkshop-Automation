page-number-372-line-number-30,"'Environment - a Docker image with Conda dependencies'
""If you're deploying MLFlow models in atch deployments, there's no need to provide a""
'scoring script and execution environment, as oth are autogenerated.'
'Learn more aout how to deploy and use atch endpoints.'
'Managed cost with autoscaling compute'
'Invoking a atch endpoint triggers an asynchronous atch inference jo. Compute'
'resources are automatically provisioned when the jo starts, and automatically de-'
'allocated as the jo completes. So you only pay for compute when you use it.'
'You can override compute resource settings (like instance count) and advanced settings'
'(like mini atch size, error threshold, and so on) for each individual atch inference jo'
"
